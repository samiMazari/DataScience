# Data Analysis with Python

| Process | Description | Code Example |
| :--- | :--- | :--- |
| **Splitting data for training and testing** | The process involves first separating the target attribute from the rest of the data. Then the target attribute as the output and the rest of the data as input. Now we split the input and output datasets into training and testing subsets. | `from sklearn.model_selection import train_test_split`<br>`y_data = df['target_attribute']`<br>`x_data=df.drop('target_attribute',axis=1)`<br>`x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=1)` |
| **Cross validation score** | Without sufficient data, you can go for cross validation, which involves creating different subsets of training and testing data multiple times and evaluating performance across all of them using the RÂ² value. | `from sklearn.model_selection import cross_val_score`<br>`from sklearn.linear_model import LinearRegression`<br>`lr = LinearRegression()`<br>`scores = cross_val_score(lr, x_data[['attribute_1', 'attribute_2', ...]], y_data, cv=4)`<br>`# cv is the number of subsets (folds), four times the cross validation is to be done`<br>`mean = scores.mean()`<br>`std = scores.std()` |
| **Cross validation predictions** | Use a cross validated model to create predictions of the output. | `from sklearn.model_selection import cross_val_predict`<br>`from sklearn.linear_model import LinearRegression`<br>`lr = LinearRegression()`<br>`yhat = cross_val_predict(lr, x_data[['attribute_1', '...']], y_data, cv=4)` |
| **Ridge Regression and Prediction** | To create a better fitting polynomial regression model, like one that avoids overfitting to the training data, you can use the Ridge regression model with a parameter alpha that is used to modify the effect of higher-order parameters on the model prediction. | `from sklearn.linear_model import Ridge`<br>`pr=PolynomialFeatures(degree=n)`<br>`pr_ridge_pipeline = Pipeline([('pr', pr), ('ridge', Ridge())])`<br>`pr_ridge_pipeline.fit(x_train, y_train)`<br>`yhat = pr_ridge_pipeline.predict(x_test)` |
| **Grid Search** | Use Grid Search to find the correct alpha value for which the Ridge regression model gives the best performance. It further uses cross-validation to create a more refined model. | `from sklearn.model_selection import GridSearchCV`<br>`parameters = [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, ...]}]`<br>`rr = Ridge()`<br>`Grid = GridSearchCV(rr, parameters, cv=4)`<br>`Grid.fit(x_data[['attribute_1', '...', 'attribute_x']], y_data)`<br>`best_rr_score = Grid.best_estimator_.score(x_data[['attribute_1', '...', 'attribute_x']], y_data)`<br>`y_predict_test=Grid.predict(x_test)` |
